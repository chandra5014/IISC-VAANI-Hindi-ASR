{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z83Vf4kfijCv",
        "outputId": "8ebcd361-125a-4b04-8964-a5abcd9dcdac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: I need to mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jro1gqw1ilYr",
        "outputId": "0c7c537d-c858-40dc-d2f1-ad4eef0e6271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "from uuid import uuid4\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "BASE_URL = \"https://asr.vaani-artpark.in\"\n",
        "API_KEY = \"GCTYVSHR5609345XCSMV\"\n",
        "LANGUAGE = \"hi\"\n",
        "INPUT_CSV = \"/content/drive/MyDrive/Tech Team Task/IISC API evaluation/New IISC Transcription from Vaani /hindi_processed - hindi_processed (2).csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Tech Team Task/IISC API evaluation/New IISC Transcription from Vaani /Transcription\"\n",
        "BATCH_SIZE = 50\n",
        "MAX_ROWS = 10000\n",
        "SKIPPED_ROWS_PATH = os.path.join(OUTPUT_DIR, \"skipped_rows.csv\")\n",
        "\n",
        "DOWNLOAD_DIR = \"downloaded_ogg\"\n",
        "CONVERTED_DIR = \"converted_wav\"\n",
        "headers = {\"x-api-key\": API_KEY}\n",
        "\n",
        "# === PREPARE FOLDERS ===\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(CONVERTED_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# === UTILITY FUNCTIONS ===\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "        print(f\"‚úÖ Downloaded: {url}\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download {url} (status {response.status_code})\")\n",
        "\n",
        "def convert_ogg_to_wav(input_path, output_path):\n",
        "    command = ['ffmpeg', '-y', '-i', input_path, '-ac', '1', '-ar', '16000', output_path]\n",
        "    subprocess.run(command, check=True)\n",
        "    print(f\"üéß Converted to WAV: {output_path}\")\n",
        "\n",
        "def enable_asr_service(wait_seconds=420):\n",
        "    print(\"‚öôÔ∏è Enabling ASR service...\")\n",
        "    response = requests.post(f\"{BASE_URL}/enable-asr\", headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"üü¢ ASR service is starting. Waiting {wait_seconds // 60} minutes...\")\n",
        "        for i in range(wait_seconds, 0, -30):\n",
        "            print(f\"‚è≥ Waiting... {i} seconds left\")\n",
        "            time.sleep(30)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Failed to enable ASR: {response.status_code} ‚Äî {response.text}\")\n",
        "        print(\"‚è≠Ô∏è Skipping ASR enabling and continuing anyway (assuming it's already active).\")\n",
        "\n",
        "def transcribe_audio(audio_file_path, language=\"hi\"):\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        files = {\n",
        "            \"file\": (os.path.basename(audio_file_path), audio_file, \"audio/wav\"),\n",
        "            \"language\": (None, language)\n",
        "        }\n",
        "        response = requests.post(f\"{BASE_URL}/transcribe\", headers=headers, files=files)\n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"transcription\", \"\")\n",
        "        else:\n",
        "            raise Exception(f\"Transcription error: {response.status_code} {response.text}\")\n",
        "\n",
        "# === MAIN PROCESSING FUNCTION ===\n",
        "\n",
        "def process_batches(input_csv, output_dir, batch_size=50, max_rows=10000):\n",
        "    df = pd.read_csv(input_csv).head(max_rows)\n",
        "    skipped_rows = []  # will collect skipped rows with reason\n",
        "\n",
        "    enable_asr_service()\n",
        "\n",
        "    for start in range(0, len(df), batch_size):\n",
        "        end = min(start + batch_size, len(df))\n",
        "        batch_df = df.iloc[start:end].copy()\n",
        "        successful_rows = []\n",
        "\n",
        "        print(f\"\\nüöÄ Processing batch {start} to {end}\")\n",
        "\n",
        "        for idx, row in batch_df.iterrows():\n",
        "            url = row.get(\"Filename\")\n",
        "            print(f\"\\nüîπ Row {idx + 1}: {url}\")\n",
        "\n",
        "            if pd.isna(url) or not isinstance(url, str) or url.strip() == \"\":\n",
        "                print(\"‚ö†Ô∏è Skipped: Missing or invalid URL\")\n",
        "                row[\"Skip_Reason\"] = \"Missing or invalid URL\"\n",
        "                skipped_rows.append(row)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                file_id = str(uuid4())\n",
        "                ogg_path = os.path.join(DOWNLOAD_DIR, f\"{file_id}.ogg\")\n",
        "                wav_path = os.path.join(CONVERTED_DIR, f\"{file_id}.wav\")\n",
        "\n",
        "                download_file(url, ogg_path)\n",
        "                convert_ogg_to_wav(ogg_path, wav_path)\n",
        "\n",
        "                transcription = transcribe_audio(wav_path, LANGUAGE)\n",
        "                print(f\"‚úÖ Transcription: {transcription}\")\n",
        "\n",
        "                row[\"Transcription\"] = transcription\n",
        "                successful_rows.append(row)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Skipping Row {idx + 1} due to error: {e}\")\n",
        "                row[\"Skip_Reason\"] = str(e)\n",
        "                skipped_rows.append(row)\n",
        "\n",
        "        # Save successful rows of this batch\n",
        "        if successful_rows:\n",
        "            output_batch = pd.DataFrame(successful_rows)\n",
        "            batch_filename = os.path.join(output_dir, f\"Transcription_batch_{start}.csv\")\n",
        "            output_batch.to_csv(batch_filename, index=False)\n",
        "            print(f\"üìÑ Saved {len(successful_rows)} rows to {batch_filename}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No successful rows in this batch.\")\n",
        "\n",
        "    # Save skipped rows at the end\n",
        "    if skipped_rows:\n",
        "        skipped_df = pd.DataFrame(skipped_rows)\n",
        "        skipped_df.to_csv(SKIPPED_ROWS_PATH, index=False)\n",
        "        print(f\"\\nüìÅ Skipped rows saved to: {SKIPPED_ROWS_PATH}\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ No rows were skipped!\")\n",
        "\n",
        "# === RUN SCRIPT ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_batches(INPUT_CSV, OUTPUT_DIR, BATCH_SIZE, MAX_ROWS)\n"
      ],
      "metadata": {
        "id": "oAY14STfonfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenizatio"
      ],
      "metadata": {
        "id": "RlzP9dNRstzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "sheet_path = \"/content/sarvam_hindi_transcription - sarvam_hindi_transcription (1).csv\"\n",
        "df = pd.read_csv(sheet_path)  # Limit to first 50 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aiUc8Q1ysNRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_corpus_reference = []\n",
        "hindi_corpus_hypothesis = []\n",
        "for _, row in df.iterrows():\n",
        "    hindi_corpus_reference.append(row['reference'])\n",
        "    hindi_corpus_hypothesis.append(row['hypothesis'])"
      ],
      "metadata": {
        "id": "8X7Reua5sal3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_corpus_reference.extend(hindi_corpus_hypothesis)\n",
        "len(hindi_corpus_reference)"
      ],
      "metadata": {
        "id": "pp-mGbhZscgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, normalizers\n",
        "from tokenizers.normalizers import NFKC, Lowercase, Sequence, Replace\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import os\n",
        "\n",
        "# Save sentences to a temporary file\n",
        "os.makedirs(\"hindi_data\", exist_ok=True)\n",
        "with open(\"hindi_data/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in hindi_corpus_reference:\n",
        "        f.write(str(line) + \"\\n\")\n",
        "\n",
        "# Initialize a tokenizer (WordLevel or BPE can be used)\n",
        "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
        "\n",
        "\n",
        "# Add Replace normalizers to remove Hindi and English sentence enders\n",
        "remove_sentence_enders = Sequence([\n",
        "    Replace(pattern=\"‡•§\", content=\"\"),\n",
        "    Replace(pattern=\"?\", content=\"\"),\n",
        "    Replace(pattern=\"#\", content=\"\"),\n",
        "    Replace(pattern=\":\", content=\"\"),\n",
        "    Replace(pattern=\"\\\\\", content=\"\"),\n",
        "    Replace(pattern=\"‚Äò\", content=\"\"),\n",
        "    # Replace(pattern=\"'\", content=\"\"),\n",
        "    # Replace(pattern=\".\", content=\"\"),\n",
        "    Replace(pattern=\",\", content=\"\"),\n",
        "    Replace(pattern=\"-\", content=\"\"),\n",
        "    Replace(pattern=\"!\", content=\"\"),\n",
        "    Replace(pattern=\"(\", content=\"\"),\n",
        "    Replace(pattern=\")\", content=\"\"),\n",
        "    Replace(pattern=\"'\", content=\"\"),\n",
        "    Replace(pattern=\"):\", content=\"\"),\n",
        "    Replace(pattern=\"='\", content=\"\"),\n",
        "    Replace(pattern=\"='\", content=\"\"),\n",
        "    Replace(pattern=\".\", content=\"\"),\n",
        "    Replace(pattern=\"%\", content=\"\"),\n",
        "    Replace(pattern=\"/\", content=\"\"),\n",
        "    Replace(pattern=\";\", content=\"\"),\n",
        "    Replace(pattern=\". \", content=\"\"),\n",
        "    Replace(pattern=\"„Ää\", content=\"\"),\n",
        "    Replace(pattern=\"„Äã\", content=\"\"),\n",
        "    Replace(pattern=\"„Éª\", content=\"\"),\n",
        "    Replace(pattern=\"„ÄÇ\", content=\"\"),\n",
        "\n",
        "\n",
        "    # Replace(pattern=\",?\", content=\"\"),\n",
        "    # Replace(pattern=\".,\", content=\"\"),\n",
        "    # Replace(pattern=\"..\", content=\"\"),\n",
        "    # Replace(pattern=\"....\", content=\"\"),\n",
        "    # Replace(pattern=\"[!?]\", content=\"\"),\n",
        "    # Replace(pattern=\"‡•§\", content=\"\"),       # Hindi danda\n",
        "    # Replace(pattern=\"[!?]\", content=\"\"),# English sentence enders\n",
        "    # Replace(pattern=\"[!]\", content=\"\"),\n",
        "    # Replace(pattern=\"[%]\", content=\"\"),\n",
        "    # Replace(pattern=\"[*]\", content=\"\"),\n",
        "    # Replace(pattern=\"[,]\", content=\"\"),\n",
        "    # Replace(pattern=\"[-]\", content=\"\"),\n",
        "    # Replace(pattern=\"[.]\", content=\"\"),\n",
        "    # Replace(pattern=\"[:]\", content=\"\"),\n",
        "    Replace(pattern=\"<\", content=\"\"),\n",
        "    Replace(pattern=\">\", content=\"\"),\n",
        "    Replace(pattern=\"ÔøΩ\", content=\"\"),\n",
        "    Replace(pattern=\"[\", content=\"\"),\n",
        "    Replace(pattern=\"]\", content=\"\"),\n",
        "    # Replace(pattern=\"[?]\", content=\"\"),\n",
        "    # Replace(pattern=\"[nan]\", content=\"\")\n",
        "    NFKC(),\n",
        "    Lowercase()\n",
        "])\n",
        "\n",
        "# Setup normalizer, pre-tokenizer, and decoder\n",
        "tokenizer.normalizer = remove_sentence_enders\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
        "\n",
        "# Trainer config\n",
        "trainer = trainers.WordLevelTrainer(\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer.train([\"hindi_data/corpus.txt\"], trainer)\n",
        "\n",
        "# Save tokenizer to JSON\n",
        "tokenizer.save(\"hindi_data_tokenizer.json\")\n",
        "print(\"‚úÖ Saved tokenizer as 'hindi_data_tokenizer.json'\")"
      ],
      "metadata": {
        "id": "EzVV53pfsfb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/content/hindi_data_tokenizer.json\")"
      ],
      "metadata": {
        "id": "LR6ewZYRsiiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "text = \"‡§ó‡•á‡§π‡•Ç‡§Ç ‡§ï‡§æ ‡§ú‡•ã ‡§¨‡•Ä‡§ú ‡§¨‡•ã‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§™‡§§‡§æ ‡§™‡•Ä‡§≤‡§æ ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à, ‡§â‡§∏‡§ï‡§æ ‡§ï‡•ã‡§à ‡§¶‡§µ‡§æ?\"\n",
        "normalized_text = unicodedata.normalize(\"NFKC\", str(text))\n",
        "print(tokenizer.tokenize(normalized_text))"
      ],
      "metadata": {
        "id": "Eix6_2q7skez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WER"
      ],
      "metadata": {
        "id": "2tsmpdBIsNuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the sheet from Google Drive\n",
        "# sheet_path = \"/content/drive/MyDrive/Tech Team Task/IISC API evaluation/IISC_WER/Vaani ASR transcription.xlsx - Sheet1.csv\"\n",
        "# df = pd.read_csv(sheet_path)  # Limit to first 50 rows\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/content/hindi_data_tokenizer.json\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(text, tokenizer):\n",
        "    normalized_text = unicodedata.normalize(\"NFKC\", str(text))\n",
        "    return tokenizer.tokenize(normalized_text)\n",
        "\n",
        "# WER with error counts\n",
        "def wer_errors(reference, hypothesis, tokenizer):\n",
        "    r = tokenize(reference, tokenizer)\n",
        "    h = tokenize(hypothesis, tokenizer)\n",
        "\n",
        "    d = np.zeros((len(r) + 1, len(h) + 1), dtype=np.uint8)\n",
        "    for i in range(len(r) + 1):\n",
        "        d[i][0] = i\n",
        "    for j in range(len(h) + 1):\n",
        "        d[0][j] = j\n",
        "\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            cost = 0 if r[i - 1] == h[j - 1] else 1\n",
        "            d[i][j] = min(\n",
        "                d[i - 1][j] + 1,      # deletion\n",
        "                d[i][j - 1] + 1,      # insertion\n",
        "                d[i - 1][j - 1] + cost  # substitution\n",
        "            )\n",
        "\n",
        "    total_errors = d[len(r)][len(h)]\n",
        "    return np.int64(total_errors), np.int64(len(r))\n",
        "\n",
        "# Initialize error counters\n",
        "total_errors = np.int64(0)\n",
        "total_reference_tokens = np.int64(0)\n",
        "\n",
        "# Loop over all rows\n",
        "i = 200\n",
        "for _, row in df.iterrows():\n",
        "  if i > 0:\n",
        "    i -= 1\n",
        "    ref = row['reference']\n",
        "    hyp = row['hypothesis']\n",
        "    print(f\"hyp-----{hyp}\")\n",
        "    print(f\"ref-----{ref}\")\n",
        "    errors, ref_len = wer_errors(ref, hyp, tokenizer)\n",
        "    total_errors += errors\n",
        "    total_reference_tokens += ref_len\n",
        "\n",
        "\n",
        "# Calculate final WER\n",
        "total_wer = (total_errors / total_reference_tokens) * 100 if total_reference_tokens > 0 else 0.0\n",
        "\n",
        "# Print final WER\n",
        "print(f\"Total WER across all rows: {total_wer:.2f}%\")\n"
      ],
      "metadata": {
        "id": "4811TXIxssJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CER"
      ],
      "metadata": {
        "id": "Fzj3x-kOEJEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# # Load the sheet from Google Drive\n",
        "# sheet_path = \"/content/Copy of Kikuyu_ASR_Report - Kikuyu_Transcription.csv\"\n",
        "# df = pd.read_csv(sheet_path)  # Limit to first 50 rows\n",
        "\n",
        "# Load the tokenizer (adjust the path to your tokenizer)\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/content/hindi_data_tokenizer.json\")\n",
        "\n",
        "# Normalize text and tokenize character-wise\n",
        "def tokenize_odiya_char(text):\n",
        "    normalized_text = unicodedata.normalize(\"NFKC\", str(text))\n",
        "    return list(normalized_text)\n",
        "\n",
        "# Levenshtein distance between two sequences\n",
        "def levenshtein_distance(r, h):\n",
        "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.int64)\n",
        "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
        "\n",
        "    for i in range(len(r) + 1):\n",
        "        d[i][0] = i\n",
        "    for j in range(len(h) + 1):\n",
        "        d[0][j] = j\n",
        "\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            cost = 0 if r[i - 1] == h[j - 1] else 1\n",
        "            d[i][j] = min(\n",
        "                d[i - 1][j] + 1,      # Deletion\n",
        "                d[i][j - 1] + 1,      # Insertion\n",
        "                d[i - 1][j - 1] + cost  # Substitution\n",
        "            )\n",
        "    return d[len(r)][len(h)]\n",
        "\n",
        "# Calculate total CER over all rows\n",
        "total_distance = 0\n",
        "total_chars = 0\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    ref = tokenize_odiya_char(row['reference'])\n",
        "    hyp = tokenize_odiya_char(row['hypothesis'])\n",
        "\n",
        "    total_distance += levenshtein_distance(ref, hyp)\n",
        "    total_chars += len(ref)\n",
        "\n",
        "total_cer = (total_distance / total_chars) * 100 if total_chars > 0 else 0.0\n",
        "\n",
        "# Print result\n",
        "print(f\"Total Character Error Rate (CER): {total_cer:.2f}%\")\n"
      ],
      "metadata": {
        "id": "OTwO2-0mEJyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MER"
      ],
      "metadata": {
        "id": "iyamzgHkEKuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# # Load the sheet from Google Drive\n",
        "# sheet_path = \"/content/drive/MyDrive/Tech Team Task/IISC API evaluation/IISC_WER/Vaani ASR transcription.xlsx - Sheet1.csv\"\n",
        "# df = pd.read_csv(sheet_path)  # Limit to first 50 rows\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/content/hindi_data_tokenizer.json\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_odiya(text, tokenizer):\n",
        "    normalized_text = unicodedata.normalize(\"NFKC\", str(text))\n",
        "    return tokenizer.tokenize(normalized_text)\n",
        "\n",
        "# Function to calculate MER errors\n",
        "def mer_errors(reference, hypothesis, tokenizer):\n",
        "    r = tokenize_odiya(reference, tokenizer)\n",
        "    h = tokenize_odiya(hypothesis, tokenizer)\n",
        "\n",
        "    # Change dtype to a larger integer type like 'int64' or 'int32' to avoid overflow\n",
        "    d = np.zeros((len(r) + 1, len(h) + 1), dtype=np.int64)\n",
        "    for i in range(len(r) + 1):\n",
        "        d[i][0] = i\n",
        "    for j in range(len(h) + 1):\n",
        "        d[0][j] = j\n",
        "\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            cost = 0 if r[i - 1] == h[j - 1] else 1\n",
        "            d[i][j] = min(\n",
        "                d[i - 1][j] + 1,      # deletion\n",
        "                d[i][j - 1] + 1,      # insertion\n",
        "                d[i - 1][j - 1] + cost  # substitution\n",
        "            )\n",
        "\n",
        "    total_errors = d[len(r)][len(h)]\n",
        "    # Ensure correct_matches is also a larger integer type\n",
        "    correct_matches = np.int64(len(r) + len(h) - total_errors)\n",
        "    return total_errors, correct_matches, len(r)\n",
        "\n",
        "# Initialize error counters\n",
        "total_errors = 0\n",
        "# total_correct_matches = 0\n",
        "total_correct_matches = np.int64(0)\n",
        "total_reference_tokens = 0\n",
        "\n",
        "# Loop over all rows\n",
        "for _, row in df.iterrows():\n",
        "    ref = row['reference']\n",
        "    hyp = row['hypothesis']\n",
        "    errors, correct_matches, ref_len = mer_errors(ref, hyp, tokenizer)\n",
        "    total_errors += errors\n",
        "    total_correct_matches += correct_matches\n",
        "    total_reference_tokens += ref_len\n",
        "\n",
        "# Calculate final MER\n",
        "total_mer = (total_errors / (total_errors + total_correct_matches)) * 100 if (total_errors + total_correct_matches) > 0 else 0.0\n",
        "\n",
        "# Print final MER\n",
        "print(f\"Total MER across all rows: {total_mer:.2f}%\")"
      ],
      "metadata": {
        "id": "SxlWb5ybEL2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}